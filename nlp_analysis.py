# -*- coding: utf-8 -*-
"""NLP Analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ppBB4CB1M_3m-idos_AB9ir2Nw0XRwEv
"""

!pip install wget

!pip install beautifulsoup4

from google.colab import drive
drive.mount('/content/drive')

!pip install simpletransformers

import zipfile
import os

zip_path = '/content/drive/MyDrive/data/archive (7).zip'
extract_path = '/content'
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

import tensorflow as tf
from transformers import TFAutoModel, AutoTokenizer

import numpy as np
import pandas as pd
import re
import os
from bs4 import BeautifulSoup #??
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from nltk.corpus import stopwords
from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional, Flatten
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.callbacks import EarlyStopping
import warnings
pd.set_option("display.max_colwidth", 200)
warnings.filterwarnings("ignore")
import wget
import nltk

data_path = "/content/Travel details dataset.csv"
data = pd.read_csv(data_path)

data.head()

#nltk.download('stopwords')  # donload the stopwords library for use by NLTK
#stop_words = set(stopwords.words('english'))

# create a function to clean the text.  Remove whitespace, html characters, punctuation etc...
#def text_cleaner(text):
   # newString = text.lower()
   # newString = BeautifulSoup(newString, "lxml").text
   # newString = re.sub(r'\([^)]*\)', '', newString)
   # newString = re.sub('"', '', newString)
    #newString = re.sub(r"'s\b", "", newString)
    #newString = re.sub("[^a-zA-Z]", " ", newString)
    #tokens = [w for w in newString.split() if not w in stop_words]
    #long_words = [i for i in tokens if len(i) >= 3]  # removing short words
   # return " ".join(long_words)

# store the cleaned text in a list
# cleaned_text = []
#for t in data['Trip ID,	Destination,	Start date	End date	Duration (days)	Traveler name	Traveler age	Traveler gender	Traveler nationality	Accommodation type	Accommodation cost	Transportation type	Transportation cost']:
#    cleaned_text.append(text_cleaner(t))

#data['Cleaned Destination'] = data['Destination'].apply(text_cleaner)
#data['Cleaned Accommodation type'] = data['Accommodation type'].apply(text_cleaner)
#data['Cleaned Transportation type'] = data['Transportation type'].apply(text_cleaner)

# Tokenize the data
model_name = "roberta-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Function to tokenize the Sentences
def tokenize_texts(texts):
  return tokenizer(texts.tolist(), padding=True, truncation=True, max_length=100, return_tensors="tf")

# Function to covert data into TensorFlow Datasets
def convert_to_tf_dataset(x, y):
    dataset = tf.data.Dataset.from_tensor_slices((tokenize_texts(x), y))
    return dataset.map(lambda x, y: ({'input_ids': x['input_ids'], 'attention_mask': x['attention_mask']}, y))

class BERTForBinaryClassification(tf.keras.Model):
    def __init__(self, bert_model, dropout_rate=0.1):
        super().__init__()
        self.bert = bert_model
        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)
        # First additional dense layer with L2 regularization
        self.dense1 = tf.keras.layers.Dense(512, activation='relu')
        self.batch_norm1 = tf.keras.layers.BatchNormalization()
        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)
        # Second additional dense layer
        self.dense2 = tf.keras.layers.Dense(128, activation='relu')
        self.batch_norm2 = tf.keras.layers.BatchNormalization()
        self.dropout3 = tf.keras.layers.Dropout(dropout_rate)
        # Final dense layer for binary classification
        self.classifier = tf.keras.layers.Dense(1, activation='sigmoid')

    def call(self, inputs):
        x = self.bert(inputs)[1]  # Get pooled output
        x = self.dropout1(x)
        x = self.dense1(x)
        x = self.batch_norm1(x)  # Apply batch normalization
        x = self.dropout2(x)
        x = self.dense2(x)
        x = self.batch_norm2(x)  # Apply batch normalization
        x = self.dropout3(x)
        return self.classifier(x)

model = TFAutoModel.from_pretrained(model_name)

classifier = BERTForBinaryClassification(model)

classifier.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),
    loss=tf.keras.losses.BinaryCrossentropy(),
    metrics=['accuracy']
)

early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=3,
    restore_best_weights=True
)

history = classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

train_texts = data['Destination'].astype(str).tolist()
train_labels = data['Traveler nationality'].astype(str).tolist()

# Assuming you have a validation set (you can split your data accordingly)
val_texts = data['Destination'].astype(str).tolist()[:100]  # Example: using first 100 samples as validation
val_labels = data['Traveler nationality'].astype(str).tolist()[:100]

# Assuming 'Destination' as train_texts and 'Traveler nationality' as train_labels for demonstration

# Preprocess your data to extract train_texts and train_labels
train_texts = data['Destination'].astype(str).tolist()  # Convert to list if not already
train_labels = data['Traveler nationality'].astype(str).tolist()  # Convert to list if not already

# Assuming you have a validation set (you can split your data accordingly)
val_texts = data['Destination'].astype(str).tolist()[:100]  # Example: using first 100 samples as validation
val_labels = data['Traveler nationality'].astype(str).tolist()[:100]

# Convert data to TF datasets
train_dataset = convert_to_tf_dataset(train_texts, train_labels)
val_dataset = convert_to_tf_dataset(val_texts, val_labels)

# Train the model
history = classifier.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=3,
    callbacks=[early_stopping]
)

train_dataset = convert_to_tf_dataset(train_texts, train_labels)
val_dataset = convert_to_tf_dataset(val_texts, val_labels)

history = classifier.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=3,
    callbacks=[early_stopping]
)